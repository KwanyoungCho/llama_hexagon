

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Block Ops Definitions &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="CPU Backend Op Definition Supplement" href="../../OpDef/CpuOpDefSupplement.html" />
    <link rel="prev" title="Usage" href="blockop_onnx_usage.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.35.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../operations.html">Operations</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../OpDef/SupportedOps.html">Supported Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OpDef/opdef_version_history.html">Op Definition Revision History</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OpDef/opdef_version_history.html#backend-specific-revision-history">Backend Specific Revision History</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations.html#operation-definitions">Operation definitions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../operations.html#block-operators">Block operators</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="blockop_overview.html">Block Op Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="blockop_usage.html">Block Op Usage</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Block Op Definitions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#buffer">Buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maskedsoftmax">MaskedSoftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statefulgru">StatefulGRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statefullstm">StatefulLSTM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../operations.html#backend-supplements">Backend supplements</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tflite_delegate.html">QNN TFLite Delegate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../revision_history.html">Revision History</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../operations.html">Operations</a> &raquo;</li>
        
      <li>Block Ops Definitions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="block-ops-definitions">
<h1>Block Ops Definitions<a class="headerlink" href="#block-ops-definitions" title="Permalink to this heading">¶</a></h1>
<p>This page documents detailed operator definitions for block ops supported by QAIRT tools.</p>
<p>Block ops use the special domain name <strong>qti_aisw</strong>.</p>
<div class="section" id="buffer">
<h2>Buffer<a class="headerlink" href="#buffer" title="Permalink to this heading">¶</a></h2>
<p>This block op accumulates inputs across inferences into a buffer of size buffer_size
and outputs the buffer with the collected inputs. When the buffer is full the oldest
existing inputs in the buffer are removed to make space for the incoming new input.
The number of inputs to remove is determined by stride. The remaining inputs are
shifted in the buffer to maintain the order they were received.</p>
<dl>
<dt><strong>Signature</strong></dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>Buffer(input data: TensorT[batch, height, width, channel],
        input reset: Boolean,
        parameter buffer_size: uint32,
        parameter buffer_dim: uint32,
        parameter stride: uint32,
        parameter mode: enum(uint32)
        → output data: TensorT[batch, height, width, channel])
</pre></div>
</div>
</dd>
<dt><strong>Inputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>inp: T (Tensor)</strong></p></td>
<td><p>Input data to be accumulated.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: A tensor of rank N</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>reset: Boolean</strong></p></td>
<td><p>The reset input determines if the buffer should be reset.</p>
<p>When set to true all frames in the buffer are removed.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Default: False</p></li>
<li><p>Shape: 0D tensor containing scalar value</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>out: T (Tensor)</strong></p></td>
<td><p>Output after inputs are accumulated.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: A tensor of rank N, same shape as inp except where dim[buffer_dim] is equal to buffer_size.</p></li>
<li><p>Constraints: Same datatype and rank as input</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Attributes/Parameters</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>buffer_size: uint32</strong></p></td>
<td><p>Determines the number of inputs that a buffer can store.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Constraints: Must be evenly divisible by Shape(inp)[buffer_dim]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>buffer_dim: uint32</strong></p></td>
<td><p>Determines the dimension that inputs are accumulated on.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Constraints: value must be in range [0, N-1]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>stride: uint32</strong></p></td>
<td><p>Determines the number of inputs to remove from the buffer when the buffer is full to make space for the new incoming input.</p>
<p>The oldest existing inputs which reside at the beginning of the buffer are removed.</p>
<p>After removal the remaining inputs are kept in the order they were received.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: BUFFER_STRIDE_DEFAULT_VAL = 1</p></li>
<li><p>Constraints: Value must be in range [1, buffer_size] and must be evenly divisible by Shape(inp)[buffer_dim]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>mode: enum (uint32)</strong></p></td>
<td><p>Determines blocking behavior. How the buffer is populated differs between the modes when the buffer is not full.</p>
<p>When the buffer is full eviction and population behavior is the same for all modes.</p>
<ul>
<li><p>0 – BLOCKING</p>
<p>Execution is stopped on the existing branch of the graph if the buffer is not full.</p>
<p>The buffer is populated from the beginning to the end.</p>
<p>For example, an empty buffer with 3 slots (0,1,2) will be populated from slot 0 to slot 2.</p>
</li>
<li><p>1 – NON_BLOCKING_LEFT</p>
<p>The existing branch of the graph will always execute regardless if the buffer is full or not.</p>
<p>The buffer is populated from the beginning to the end.</p>
<p>For example, an empty buffer with 3 slots (0,1,2) will be populated from slot 0 to slot 2.</p>
</li>
<li><p>2 – NON_BLOCKING_RIGHT</p>
<p>The existing branch of the graph will always execute regardless if the buffer is full or not.</p>
<p>The buffer is populated from the end.</p>
<p>For example, an empty buffer with 3 slots (0,1,2) the first incoming input is placed at slot 2.</p>
<p>For the next incoming input the previous input at slot 2 is now at slot 1 and the new input is placed at slot 2.</p>
</li>
</ul>
<p>When the buffer is full the number of inputs removed is determined by stride.</p>
<p>Eviction behavior is the same for all modes where the oldest existing inputs are removed from the beginning of the
buffer.</p>
<p>Population behavior is also the same for all modes when the buffer is full.</p>
<p>For example, a fully populated buffer with 3 slots (0,1,2) and a stride value of 2 will have the inputs at
slot 0 and slot 1 removed and the input at slot 2 will now be at slot 0.</p>
<p>The incoming input is then placed at slot 1.</p>
<p>The next incoming input will then be placed at slot 2.</p>
<p>When the buffer is full again the same process is repeated.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NON_BLOCKING_LEFT and NON_BLOCKING_RIGHT are zero filled for the output if the buffer is not completely full.</p>
</div>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: BUFFER_STRIDE_DEFAULT_VAL = BUFFER_MODE_BLOCKING</p></li>
<li><p>Values:</p>
<ul>
<li><p>BUFFER_MODE_BLOCKING = 0,</p></li>
<li><p>BUFFER_MODE_NON_BLOCKING_LEFT = 1,</p></li>
<li><p>BUFFER_MODE_NON_BLOCKING_RIGHT = 2</p></li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Type Constraints</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>TypeName</p></th>
<th class="head"><p>Datatypes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>T</strong></p></td>
<td><p><strong>tensor(float32)</strong></p></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Supported Backends</strong></dt><dd><ul class="simple">
<li><p>QNN-CPU</p></li>
<li><p>QNN-LPAI</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is not feasible to mimic the Buffer block op in ONNX Runtime as it has blocking and stateful behavior. Hence, it is implemented as an identity op in ONNX Runtime and the output of this op may not match the expected output in ONNX Runtime.</p>
</div>
</div>
<div class="section" id="maskedsoftmax">
<h2>MaskedSoftmax<a class="headerlink" href="#maskedsoftmax" title="Permalink to this heading">¶</a></h2>
<p>This block op applies a Softmax operation on masked portions of the input tensor. For each batch the
mask tensor is broadcast on the input before softmax computation. A mask tensor must be provided in
either an UNCOMPRESSED or COMPRESSED format depending on the parameter mode selected. See input mask
for details on how a boolean mask can be converted to an UNCOMPRESSED or COMPRESSED mask tensor.</p>
<dl>
<dt><strong>Signature</strong></dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>MaskedSoftmax(input data: TensorT[batch, height, width, channel],
            input mask: TensorT1[batch, M],
            parameter mode: enum(int)
            → output data: TensorT[batch, height, width, channel])
</pre></div>
</div>
</dd>
<dt><strong>Inputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>data: T (Tensor)</strong></p></td>
<td><p>Input data to be masked and softmax to be applied to.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Constraints: When parameter mode is COMPRESSED, width == channel</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>mask: T1 (Tensor)</strong></p></td>
<td><p>The representation of this 2D tensor is determined by the parameter mode selected.</p>
<p>When parameter mode is set to UNCOMPRESSED, then M = channel or when set to COMPRESSED, then
M = number of sequences.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Constraints: When parameter mode is set to COMPRESSED the sum of the values in each batch
must be less than or equal to channel.</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Consider a boolean mask where a mask value of 1 indicates the dimension on which Softmax should
be performed and a mask value of 0 indicates the dimension that Softmax will not be performed.
An uncompressed mask can be made from a boolean mask tensor by adding -1 or subtracting by 1
element-wise and multiplying the intermediate result element-wise by a large value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">uncompressed_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">.+</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">.*</span> <span class="mi">10000</span>
<span class="o">//</span> <span class="n">uncompressed_mask</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">10000</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
<p>A compressed mask can be made from multiple boolean mask tensors of vector lengths that are concatenated
into a single batch and summed across the 2nd axis. Consider the following:</p>
<p>Let there be 3 mask tensors that correspond to sequences of inputs that were used to make in[0] where 0’s
represent where padding was added to make them the max sequence length.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">mask2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">mask3</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>The concatenated mask would then be the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">concatenated_mask</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
<p>The compressed mask representation would be made from summing across the 2nd axis:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compressed_mask</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]]</span>
</pre></div>
</div>
</dd>
<dt><strong>Outputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>out: T (Float32 Tensor)</strong></p></td>
<td><p>Output after input is masked and softmax to be applied.</p></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Attributes/Parameters</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>mode: enum(int)</strong></p></td>
<td><p>Determines the format of the input mask. See input mask for details on the format of the mask tensor.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Default: UNCOMPRESSED</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Type Constraints</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>TypeName</p></th>
<th class="head"><p>Datatypes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>T</strong></p></td>
<td><p><strong>tensor(float32)</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>T1</strong></p></td>
<td><p><strong>tensor(int32)</strong></p></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Supported Backends</strong></dt><dd><ul class="simple">
<li><p>QNN-CPU</p></li>
<li><p>QNN-AIC</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="statefulgru">
<h2>StatefulGRU<a class="headerlink" href="#statefulgru" title="Permalink to this heading">¶</a></h2>
<p>This Block op computes an one-layer GRU.</p>
<p><strong>Equations:</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}{\Large z_t = f(X_t*(Wz^T) + H_{t-1}*(Rz^{T}) + Wbz + Rbz)} \\ \\\end{split}\\\begin{split}{\Large r_t = f(X_t*(Wr^T) + H_{t-1}*(Rr^T) + Wbr + Rbr)} \\ \\\end{split}\\\begin{split}{\Large h_{t} = g(X_t*(Wh^T) + (r_t \odot H_{t-1})*(Rh^T) + Rbh + Wbh) \text{, When linear_before_reset = 0}} \\ \\\end{split}\\\begin{split}{\Large h_{t} = g(X_t*(Wh^T) + (r_t \odot (H_{t-1}*(Rh^T) + Rbh)) + Wbh) \text{, When linear_before_reset != 0}} \\ \\\end{split}\\\begin{split}{\Large H_t = (1 - z_t) \odot h_t + z_t \odot H_{t-1}} \\ \\\end{split}\end{aligned}\end{align} \]</div>
<p>Where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_{t}\)</span> - input tensor</p></li>
<li><p><span class="math notranslate nohighlight">\(z_{t}\)</span> - update gate</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{t}\)</span> - reset gate</p></li>
<li><p><span class="math notranslate nohighlight">\(h_{t}\)</span> - hidden gate</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span> - time step (<span class="math notranslate nohighlight">\(t-1\)</span> means previous time step)</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> - sigmoid activation function</p></li>
<li><p><span class="math notranslate nohighlight">\(g\)</span> - tanh activation function</p></li>
<li><p>W[zrh] - W parameter weight matrix for update, reset, and hidden gates</p></li>
<li><p>R[zrh] - R recurrence weight matrix for update, reset, and hidden gates</p></li>
<li><p>Wb[zrh] - W bias vectors for update, reset, and hidden gates</p></li>
<li><p>Rb[zrh] - R bias vectors for update, reset, and hidden gates</p></li>
<li><p>WB[zrh] - W parameter weight matrix for backward update, reset, and hidden gates</p></li>
<li><p>RB[zrh] - R recurrence weight matrix for backward update, reset, and hidden gates</p></li>
<li><p>WBb[zrh] - W bias vectors for backward update, reset, and hidden gates</p></li>
<li><p>RBb[zrh] - R bias vectors for backward update, reset, and hidden gates</p></li>
<li><p>H - Hidden state</p></li>
<li><p>num_directions - 2 if direction == bidirectional else 1</p></li>
<li><p><span class="math notranslate nohighlight">\(\odot\)</span> - element-wise product of two vectors.</p></li>
</ul>
<p><strong>Activation functions:</strong></p>
<ul class="simple">
<li><p>Tanh(x) - <span class="math notranslate nohighlight">\((1 - e^{-2x})/(1 + e^{-2x})\)</span></p></li>
<li><p>Sigmoid(x) - <span class="math notranslate nohighlight">\(1/(1 + e^{-x})\)</span></p></li>
</ul>
<p><strong>References:</strong></p>
<p>ONNX: <a class="reference external" href="https://onnx.ai/onnx/operators/onnx__GRU.html#gru-7">ops::GRU</a></p>
<p><strong>Signature</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>StatefulGru(
        input X: TensorT[seq_length, batch_size, input_size],
        input W: TensorT[num_directions, 3*hidden_size, input_size],
        input R: TensorT[num_directions, 3*hidden_size, hidden_size],
        parameter hidden_size: int,
        input B: TensorT[num_directions, 6*hidden_size] = None,
        input sequence_lens: TensorT1[batch_size] = None,
        input initial_h: TensorT[num_directions, batch_size, hidden_size] = None,
        input reset: BOOL = False,
        parameter clip: float = None,
        parameter direction: str = &quot;forward&quot;,
        parameter linear_before_reset: int = 0,
    )
    → output Y: TensorT[seq_length, num_directions, batch_size, hidden_size],
    output Y_h: TensorT[num_directions, batch_size, hidden_size]
</pre></div>
</div>
<dl>
<dt><strong>Inputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>X: T (Tensor)</strong></p></td>
<td><p>The input sequence.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [seq_length, batch_size, input_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>W: T (Tensor)</strong></p></td>
<td><p>The weight tensor for the gates. Concatenation of W[zrh] and WB[zrh] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [num_directions, 3*hidden_size, input_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>R: T (Tensor)</strong></p></td>
<td><p>The recurrence weight tensor. Concatenation of R[zrh] and RB[zrh] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [num_directions, 3*hidden_size, hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>B: T (Tensor)</strong></p></td>
<td><p>The bias tensor for input gate. Concatenation of [Wb[zrh], Rb[zrh]] and [WBb[zrh], RBb[zrh]] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Optional: If not specified, values are assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, 6*hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>sequence_lens : T1 (Tensor)</strong></p></td>
<td><p>Optional tensor specifying lengths of the sequences in a batch.</p>
<ul class="simple">
<li><p>Optional: If not specified, all sequences in the batch are assumed to have length seq_length</p></li>
<li><p>Shape: a tensor of shape [batch_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>initial_h: T (Tensor)</strong></p></td>
<td><p>Optional initial value of the hidden.</p>
<ul class="simple">
<li><p>Optional: If not specified, values are assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, batch_size, hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>reset: Boolean</strong></p></td>
<td><p>Determines if the internal state should be reset from the beginning of an inference pass.</p>
<p>When set to true the internal state is reset by the input initial_h if it is provided, otherwise it is set to all zero values.</p>
<p>This input is used to indicate the reset of the internal state at the beginning of an inference pass across all batch elements at time-step 0.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Default: False</p></li>
<li><p>Shape: 0D tensor containing scalar value</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The reset input is ignored in ONNX Runtime and the behavior is same as when reset is False.</p>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Y: T (Tensor)</strong></p></td>
<td><p>A tensor that concats all the intermediate output values of the hidden.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: a tensor of shape [seq_length, num_directions, batch_size, hidden_size].</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>Y_h: T (Tensor)</strong></p></td>
<td><p>The last output value of the hidden.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: a tensor of shape [num_directions, batch_size, hidden_size].</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Attributes/Parameters</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>hidden_size: int</strong></p></td>
<td><p>Number of neurons in the hidden layer</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: Scalar</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>clip: float</strong></p></td>
<td><p>Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>direction: str</strong></p></td>
<td><p>Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: “forward”</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>linear_before_reset: int</strong></p></td>
<td><p>When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: 0</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Type Constraints</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>TypeName</p></th>
<th class="head"><p>Datatypes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>T</strong></p></td>
<td><p><strong>tensor(float32)</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>T1</strong></p></td>
<td><p><strong>tensor(int32)</strong></p></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Supported Backends</strong></dt><dd><ul class="simple">
<li><p>QNN-CPU</p></li>
<li><p>QNN-LPAI</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="statefullstm">
<h2>StatefulLSTM<a class="headerlink" href="#statefullstm" title="Permalink to this heading">¶</a></h2>
<p>This Block op computes an one-layer LSTM.</p>
<p><strong>Equations:</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}{\Large i_t = f(X_t*(Wi^T) + H_{t-1}*(Ri^T) + Pi \odot C_{t-1} + Wbi + Rbi)} \\ \\\end{split}\\\begin{split}{\Large f_t = f(X_t*(Wf^T) + H_{t-1}*(Rf^T) + Pf \odot C_{t-1} + Wbf + Rbf)} \\ \\\end{split}\\\begin{split}{\Large c_t = g(X_t*(Wc^T) + H_{t-1}*(Rc^T) + Wbc + Rbc)} \\ \\\end{split}\\\begin{split}{\Large C_t = f_t \odot C_{t-1} + i_t \odot c_t} \\ \\\end{split}\\\begin{split}{\Large o_t = f(X_t*(Wo^T) + H_{t-1}*(Ro^T) + Po \odot C_t + Wbo + Rbo)} \\ \\\end{split}\\\begin{split}{\Large H_t = o_t \odot h(C_t)} \\ \\\end{split}\end{aligned}\end{align} \]</div>
<p>Where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_{t}\)</span> - input tensor</p></li>
<li><p><span class="math notranslate nohighlight">\(i_{t}\)</span> - input gate</p></li>
<li><p><span class="math notranslate nohighlight">\(o_{t}\)</span> - output gate</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{t}\)</span> - forget gate</p></li>
<li><p><span class="math notranslate nohighlight">\(c_{t}\)</span> - cell gate</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span> - time step (<span class="math notranslate nohighlight">\(t-1\)</span> means previous time step)</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> - sigmoid activation function</p></li>
<li><p><span class="math notranslate nohighlight">\(g\)</span> - tanh activation function</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> - tanh activation function</p></li>
<li><p>W[iofc] - W parameter weight matrix for input, output, forget, and cell gates</p></li>
<li><p>R[iofc] - R recurrence weight matrix for input, output, forget, and cell gates</p></li>
<li><p>Wb[iofc] - W bias vectors for input, output, forget, and cell gates</p></li>
<li><p>Rb[iofc] - R bias vectors for input, output, forget, and cell gates</p></li>
<li><p>P[iof] - P peephole weight vector for input, output, and forget gates</p></li>
<li><p>WB[iofc] - W parameter weight matrix for backward input, output, forget, and cell gates</p></li>
<li><p>RB[iofc] - R recurrence weight matrix for backward input, output, forget, and cell gates</p></li>
<li><p>WBb[iofc] - W bias vectors for backward input, output, forget, and cell gates</p></li>
<li><p>RBb[iofc] - R bias vectors for backward input, output, forget, and cell gates</p></li>
<li><p>PB[iof] - P peephole weight vector for backward input, output, and forget gates</p></li>
<li><p>H - Hidden state</p></li>
<li><p>num_directions - 2 if direction == bidirectional else 1</p></li>
<li><p><span class="math notranslate nohighlight">\(\odot\)</span> - element-wise product of two vectors.</p></li>
</ul>
<p><strong>Activation functions:</strong></p>
<ul class="simple">
<li><p>Tanh(x) - <span class="math notranslate nohighlight">\((1 - e^{-2x})/(1 + e^{-2x})\)</span></p></li>
<li><p>Sigmoid(x) - <span class="math notranslate nohighlight">\(1/(1 + e^{-x})\)</span></p></li>
</ul>
<p><strong>References:</strong></p>
<p>ONNX: <a class="reference external" href="https://onnx.ai/onnx/operators/onnx__LSTM.html#lstm-7">ops::LSTM</a></p>
<dl>
<dt><strong>Signature</strong></dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>StatefulLstm(
        input X: TensorT[seq_length, batch_size, input_size],
        input W: TensorT[num_directions, 4*hidden_size, input_size],
        input R: TensorT[num_directions, 4*hidden_size, hidden_size],
        parameter hidden_size: int,
        input B: TensorT[num_directions, 8*hidden_size] = None,
        input sequence_lens: TensorT1[batch_size] = None,
        input initial_h: TensorT[num_directions, batch_size, hidden_size] = None,
        input initial_c: TensorT[num_directions, batch_size, hidden_size] = None,
        input P: TensorT[num_directions, 3*hidden_size] = None,
        input reset: BOOL = False,
        parameter clip: float = None,
        parameter direction: str = &quot;forward&quot;,
        parameter input_forget: int = 0,
    )
    → output Y: TensorT[seq_length, num_directions, batch_size,hidden_size],
    output Y_h: TensorT[num_directions, batch_size, hidden_size],
    output Y_c: TensorT[num_directions, batch_size, hidden_size]
</pre></div>
</div>
</dd>
<dt><strong>Inputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>X: T (Tensor)</strong></p></td>
<td><p>The input sequence.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [seq_length, batch_size, input_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>W: T (Tensor)</strong></p></td>
<td><p>The weight tensor for the gates. Concatenation of W[iofc] and WB[iofc] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [num_directions, 4*hidden_size, input_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>R: T (Tensor)</strong></p></td>
<td><p>The recurrence weight tensor. Concatenation of R[iofc] and RB[iofc] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: a tensor of shape [num_directions, 4*hidden_size, hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>B: T (Tensor)</strong></p></td>
<td><p>The bias tensor for input gate. Concatenation of [Wb[iofc], Rb[iofc]], and [WBb[iofc], RBb[iofc]] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Optional: If not specified - assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, 8*hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>sequence_lens : T1 (Tensor)</strong></p></td>
<td><p>Optional tensor specifying lengths of the sequences in a batch.</p>
<ul class="simple">
<li><p>Optional: If not specified - assumed all sequences in the batch to have length seq_length</p></li>
<li><p>Shape: a tensor of shape [batch_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>initial_h: T (Tensor)</strong></p></td>
<td><p>Optional initial value of the hidden.</p>
<ul class="simple">
<li><p>Optional: If not specified - assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, batch_size, hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>initial_c: T (Tensor)</strong></p></td>
<td><p>Optional initial value of the cell.</p>
<ul class="simple">
<li><p>Optional: If not specified - assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, batch_size, hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>P: T (Tensor)</strong></p></td>
<td><p>The weight tensor for peepholes. Concatenation of P[iof] and PB[iof] (if bidirectional) along dimension 0.</p>
<ul class="simple">
<li><p>Optional: If not specified - assumed to be 0.</p></li>
<li><p>Shape: a tensor of shape [num_directions, 3*hidden_size]</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>reset: Boolean</strong></p></td>
<td><p>Determines if the internal cell and hidden state should be reset from the beginning of an inference pass.</p>
<p>If set to true, internal states are reset to the values provided the inputs initial_h and initial_c.</p>
<p>If set to true, and initial_h and initial_c are not provided, internal states are set to all zero values.</p>
<p>This input is used to indicate the reset of the internal states at the beginning of an inference pass across all batch elements at time-step 0.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Default: False</p></li>
<li><p>Shape: 0D tensor containing scalar value</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The reset input is ignored in ONNX Runtime and the behavior is same as when reset is False.</p>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Y: T (Tensor)</strong></p></td>
<td><p>A tensor that concats all the intermediate output values of the hidden.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: a tensor of shape [seq_length, num_directions, batch_size, hidden_size].</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>Y_h: T (Tensor)</strong></p></td>
<td><p>The last output value of the hidden.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: a tensor of shape [num_directions, batch_size, hidden_size].</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>Y_c: T (Tensor)</strong></p></td>
<td><p>The last output value of the cell.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: a tensor of shape [seq_length, num_directions, batch_size, hidden_size].</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Attributes/Parameters</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>hidden_size: int</strong></p></td>
<td><p>Number of neurons in the hidden layer</p>
<ul class="simple">
<li><p>Mandatory</p></li>
<li><p>Shape: Scalar</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>clip: float</strong></p></td>
<td><p>Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><strong>direction: str</strong></p></td>
<td><p>Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: “forward”</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><strong>input_forget: int</strong></p></td>
<td><p>Couple the input and forget gates if 1.</p>
<ul class="simple">
<li><p>Optional</p></li>
<li><p>Shape: Scalar</p></li>
<li><p>Default: 0</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Type Constraints</strong></dt><dd><table class="docutils align-left">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>TypeName</p></th>
<th class="head"><p>Datatypes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>T</strong></p></td>
<td><p><strong>tensor(float32)</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>T1</strong></p></td>
<td><p><strong>tensor(int32)</strong></p></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Supported Backends</strong></dt><dd><ul class="simple">
<li><p>QNN-CPU</p></li>
<li><p>QNN-LPAI</p></li>
</ul>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../../OpDef/CpuOpDefSupplement.html" class="btn btn-neutral float-right" title="CPU Backend Op Definition Supplement" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="blockop_onnx_usage.html" class="btn btn-neutral float-left" title="Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2025, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>