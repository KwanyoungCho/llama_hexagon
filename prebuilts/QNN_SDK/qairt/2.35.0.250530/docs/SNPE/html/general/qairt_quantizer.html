

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Qairt Quantizer &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Tips" href="usergroup4.html" />
    <link rel="prev" title="Qairt Converter" href="qairt_converter.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.35.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="usergroup1.html">Network Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="network_layers.html">Supported Network Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_onnx_ops.html">Supported ONNX Ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantized_models.html">Quantized vs Non-Quantized Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="usergroup2.html">User-defined Operations</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="usergroup3.html">Model Conversion</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="model_conv_tensorflow.html">TensorFlow Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorflow_graphs.html">Tensorflow Graph Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_tflite.html">TFLite Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_pytorch.html">PyTorch Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_onnx.html">ONNX Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conversion.html">Quantizing a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="offline_graph_caching.html">Offline Graph Caching for DSP Runtime on HTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="qairt_converter.html">Qairt Converter</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Qairt Quantizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#additional-details">Additional details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="usergroup4.html">Model Tips</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="usergroup1.html">Network Models</a> &raquo;</li>
        
          <li><a href="usergroup3.html">Model Conversion</a> &raquo;</li>
        
      <li>Qairt Quantizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="qairt-quantizer">
<h1>Qairt Quantizer<a class="headerlink" href="#qairt-quantizer" title="Permalink to this heading">¶</a></h1>
<p>The <a class="reference external" href="tools.html#qairt-converter">qairt-converter</a> tool now converts non-quantized models into a non-quantized or quantized
DLC file depending on the overrides provided during the Converter step. <code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> now can be used to quantize all the tensors which
are missing encodings during <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code> step (fill in the gaps) or can be used to calibrate the provided encodings through a list of images.
The <a class="reference external" href="tools.html#qairt-quantizer">qairt-quantizer</a> tool is used to quantize the model to one of supported fixed point formats.</p>
<p>For example, the following command will convert an Inception v3 DLC file into a quantized Inception v3 DLC file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ qairt-quantizer --input_dlc inception_v3.dlc \
                  --input_list image_file_list.txt \
                  --output_dlc inception_v3_quantized.dlc
</pre></div>
</div>
<p>To properly calculate the ranges for the quantization parameters, a representative set of input data needs to be used as
input into <a class="reference external" href="tools.html#qairt-quantizer">qairt-quantizer</a> using the <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> parameter.
The <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> specifies paths to raw image files to be used for calibration during quantization.
For details refer to <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> argument in <a class="reference external" href="tools.html#qnn-net-run">qnn-net-run</a> for supported
input formats (in order to calculate output activation encoding information for all layers, <strong>do not</strong> include the line
which specifies desired outputs).</p>
<p>The tool requires the batch dimension of the DLC input file to be set to 1 during model conversion. The batch dimension
can be changed to a different value for inference, by <a class="reference external" href="network_resize.html">resizing</a> the network during initialization.</p>
<div class="section" id="additional-details">
<h2>Additional details<a class="headerlink" href="#additional-details" title="Permalink to this heading">¶</a></h2>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> is majorly similar to <code class="docutils literal notranslate"><span class="pre">snpe-dlc-quant</span></code> with the following differences:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> can now be used to generate encodings using calibration dataset provided via the <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> flag
for the tensors for the following scenarios:</p>
<ul class="simple">
<li><p>Fill in the gaps: If any tensor is missing encoding during the <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code> step i.e. the tensors for which override
is not specified in <code class="docutils literal notranslate"><span class="pre">--quantization_overrides</span></code> or source model encodings (QAT).</p></li>
<li><p>If encodings is not specified for all the tensors via overrides or QAT encodings.</p></li>
</ul>
</li>
<li><p>The external overrides and source model encodings (QAT) are now applied during <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code> stage by default.
So the quantizer options to ignore the overrides and source model encodings, <code class="docutils literal notranslate"><span class="pre">--ignore_encodings</span></code> (legacy) and <code class="docutils literal notranslate"><span class="pre">--ignore_quantization_overrides</span></code> are now no-op.</p></li>
<li><p>An alternative option is to the <code class="docutils literal notranslate"><span class="pre">--export_format=DLC_STRIP_QUANT</span></code> flag of <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code>, when specified the converter will ignore/remove all the encodings in
the source model and output float model which can be recalibrated using <code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> and <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> flag.</p></li>
<li><p>Another alternative for using this feature is through <code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> options <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> and <code class="docutils literal notranslate"><span class="pre">--ignore_quantization_overrides``in</span> <span class="pre">combination</span>
<span class="pre">which</span> <span class="pre">signals</span> <span class="pre">the</span> <span class="pre">quantizer</span> <span class="pre">to</span> <span class="pre">ignores</span> <span class="pre">all</span> <span class="pre">the</span> <span class="pre">encodings</span> <span class="pre">applied</span> <span class="pre">during</span> <span class="pre">conversion</span> <span class="pre">and</span> <span class="pre">generates</span> <span class="pre">encodings</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">calibration</span> <span class="pre">dataset</span> <span class="pre">provided</span> <span class="pre">via</span> <span class="pre">``--input_list</span></code>.</p></li>
<li><p>The float fallback feature controlled via command-line option <code class="docutils literal notranslate"><span class="pre">--enable_float_fallback</span></code>, present as <code class="docutils literal notranslate"><span class="pre">--float_fallback</span></code> in legacy quantizers
is also a no-op for <code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> and can be skipped. The float fallback was added to produce a fully quantized or mixed precision graph by applying encoding overrides
or source model encodings, by propagating encodings across data invariant Ops and falling back the missing tensors to float datatype.
To simplify the steps, this is taken care during <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code>. <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code> applies the overrides and encodings, and the tensors which are missing
encodings will fall back to the default float datatype.</p></li>
<li><p>To summarize, <code class="docutils literal notranslate"><span class="pre">qairt-quantizer</span></code> command-line arguments <code class="docutils literal notranslate"><span class="pre">--ignore_quantization_overrides</span></code>, and <code class="docutils literal notranslate"><span class="pre">--enable_float_fallback</span></code> are now no-op,
and are applied by default during <code class="docutils literal notranslate"><span class="pre">qairt-converter</span></code> step itself.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">--enable_float_fallback</span></code> and <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> are mutually exclusive options. One of them is
mandatory argument for quantizer.</p>
</div>
</li>
</ul>
</li>
<li><p>Outputs can be specified for qairt-quantizer by modifying the input_list in the following ways:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified individually, but when specifying both, the order shown must
be used to specify each.</p>
</li>
<li><p>qairt-quantizer also supports quantization using AIMET, inplace of default Quantizer,
when <code class="docutils literal notranslate"><span class="pre">--use_aimet_quantizer</span></code> command line option is provided. To use AIMET Quantizer,
run the setup script to create AIMET specific environment, by executing the following command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ source {SNPE_ROOT}/bin/aimet_env_setup.sh --env_path &lt;path where AIMET venv needs to be created&gt; \
                                            --aimet_sdk_tar &lt;AIMET Torch SDK tarball&gt;
</pre></div>
</div>
</li>
<li><p>Advance AIMET algorithms- AdaRound and AMP is also supported in qairt-quantizer. The user needs to provide a YAML
config file through the command line option <code class="docutils literal notranslate"><span class="pre">--config</span></code> and specify the algorithm “adaround” or “amp” through <code class="docutils literal notranslate"><span class="pre">--apply_algorithms</span></code>
along with <code class="docutils literal notranslate"><span class="pre">--use_aimet_quantizer</span></code> flag.</p></li>
<li><p>The template of the YAML file for AMP is shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aimet_quantizer</span><span class="p">:</span>
   <span class="n">datasets</span><span class="p">:</span>
       <span class="o">&lt;</span><span class="n">dataset_name</span><span class="o">&gt;</span><span class="p">:</span>
           <span class="n">dataloader_callback</span><span class="p">:</span> <span class="s1">&#39;&lt;path/to/unlabled/dataloader/callback/function&gt;&#39;</span>
           <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="p">{</span><span class="n">arg1</span><span class="p">:</span> <span class="n">val</span><span class="p">,</span> <span class="n">arg2</span><span class="p">:</span> <span class="n">val2</span><span class="p">}</span>

   <span class="n">amp</span><span class="p">:</span>
       <span class="n">dataset</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">dataset_name</span><span class="o">&gt;</span><span class="p">,</span>
       <span class="n">candidates</span><span class="p">:</span>  <span class="p">[[[</span><span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;int&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;int&#39;</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;float&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;float&#39;</span><span class="p">]]],</span>
       <span class="n">allowed_accuracy_drop</span><span class="p">:</span> <span class="mf">0.02</span>
       <span class="n">eval_callback_for_phase2</span><span class="p">:</span> <span class="s1">&#39;&lt;path/to/evaluator/callback/function&gt;&#39;</span>
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p><em>dataloader_callback</em> is used to set the path of a callback function which returns labeled dataloader of type torch.DataLoader.
The data should be in source network input format. <em>dataloader_kwargs</em> is an optional dictionary through which the user
can provide keyword arguments of the above defined callback function. <em>dataset</em> is used to specify the name of the dataset
that has been defined above. <em>candidates</em> is list of lists for all possible bitwidth values for activations and parameters.
<em>allowed_accuracy_drop</em> is used to specify the maximum allowed drop in accuracy from FP32 baseline. The pareto front
curve is plotted only till the point where the allowable accuracy drop is met. <em>eval_callback_for_phase2</em> is used to set
the path of the evaluator function which takes predicted value batch as the first argument and ground truth batch as the
second argument and returns calculated metric float value.</p>
</div></blockquote>
<ul>
<li><p>The template of the YAML file for AdaRound is shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aimet_quantizer</span><span class="p">:</span>
    <span class="n">datasets</span><span class="p">:</span>
        <span class="o">&lt;</span><span class="n">dataset_name</span><span class="o">&gt;</span><span class="p">:</span>
            <span class="n">dataloader_callback</span><span class="p">:</span> <span class="s1">&#39;&lt;path/to/unlabled/dataloader/callback/function&gt;&#39;</span>
            <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="p">{</span><span class="n">arg1</span><span class="p">:</span> <span class="n">val</span><span class="p">,</span> <span class="n">arg2</span><span class="p">:</span> <span class="n">val2</span><span class="p">}</span>

    <span class="n">adaround</span><span class="p">:</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">dataset_name</span><span class="o">&gt;</span>
        <span class="n">num_batches</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p><em>dataloader_callback</em> is used to set the path of a callback function which returns unlabeled dataloader of type torch.DataLoader.
The data should be in source network input format. <em>dataloader_kwargs</em> is an optional dictionary through which the user
can provide keyword arguments of the above defined callback function. <em>dataset</em> is used to specify the name of the dataset
that has been defined above. <em>num_batches</em> is used to specify the number of batches to be used for adaround iteration.</p>
</div></blockquote>
<ul>
<li><p>AdaRound can also run in default mode, without config file, by just passing “adaround”
in the command line option <code class="docutils literal notranslate"><span class="pre">--apply_algorithms</span></code> along with <code class="docutils literal notranslate"><span class="pre">--use_aimet_quantizer</span></code> flag. This flow uses the data provided
through the input_list option to take rounding decisions.</p>
<dl class="simple">
<dt><strong>Note:</strong></dt><dd><ol class="arabic simple">
<li><p>AIMET Torch Tarball naming convention should be as follows -
aimetpro-release-&lt;VERSION (optionally with build ID)&gt;.torch-&lt;cpu/gpu&gt;-.*.tar.gz.
For example, aimetpro-release-x.xx.x.torch-xxx-release.tar.gz.</p></li>
<li><p>Once the setup script is run, ensure that AIMET_ENV_PYTHON environment variable is set to
&lt;AIMET virtual environment path&gt;/bin/python</p></li>
<li><p>Minimum AIMET version supported is, <strong>AIMET-1.33.0</strong></p></li>
</ol>
</dd>
</dl>
</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="usergroup4.html" class="btn btn-neutral float-right" title="Model Tips" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="qairt_converter.html" class="btn btn-neutral float-left" title="Qairt Converter" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>