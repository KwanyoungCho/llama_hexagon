

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>qnn-genai-transformer-composer &mdash; Qualcomm® Gen AI Inference Extensions</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="genie-app" href="genie-app.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Qualcomm® Gen AI Inference Extensions
          

          
          </a>

          
            
            
              <div class="version">
                v2.35.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support/support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../library/library.html">Library</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tools.html">Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="genie-t2t-run.html">genie-t2t-run</a></li>
<li class="toctree-l2"><a class="reference internal" href="genie-t2e-run.html">genie-t2e-run</a></li>
<li class="toctree-l2"><a class="reference internal" href="genie-app.html">genie-app</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">qnn-genai-transformer-composer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#generic-configuration-file-explanation">Generic Configuration File Explanation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-params-description">Model Params Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-tensor-description">Model Tensor Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rope-scaling-config-description">RoPE Scaling Config Description</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Qualcomm® Gen AI Inference Extensions</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="tools.html">Tools</a> &raquo;</li>
        
      <li>qnn-genai-transformer-composer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="qnn-genai-transformer-composer">
<span id="id1"></span><h1>qnn-genai-transformer-composer<a class="headerlink" href="#qnn-genai-transformer-composer" title="Permalink to this heading">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">qnn-genai-transformer-composer</span></code> tool prepares a model for inference via Genie, but specifically for the Gen AI
Transformer Backend.</p>
<div class="figure align-center" id="id3">
<img alt="GenAITransformer Composer diagram." src="../../_images/gen_ai_transformer_composer_diagram.png" />
<p class="caption"><span class="caption-text">GenAITransformer Composer Tool</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool to convert a supported LLM model to a binary file consumable by Genie execution engine.

REQUIRED ARGUMENTS:
-------------------
--model                        &lt;DIR&gt;           Path to the downloaded LLM model directory.

OPTIONAL ARGUMENTS:
-------------------

-h, --help                                    Show this help message and exit.

--config_file                  &lt;DIR&gt;          Path to the generic configuration.json

--quantize {Z4,Z4_FP16,Z8,Q4}  &lt;VAL&gt;          Quantization type.  If not specified, output format will be FP32.
                                              Q4 uses a block of 32 elements. It provides the highest accuracy,
                                              yet with lowest performance. Z4 and Z8 uses block of 128 elements.
                                              The accuracy is sufficient for most models, with Z8 giving highest
                                              performance.

--export_tokenizer_json        &lt;VAL&gt;          Export the tokenizer model to HuggingFace Fast Tokenizer .json file.
                                              The tokenizer.json file will be written to the path specified via the
                                              --outfile parameter.

--outfile OUTFILE              &lt;FILE&gt;         Path to write to; default: path provided in --model parameter.

--lora                         &lt;DIR&gt;          Path to the lora adapter model directory, if specified then --quantize
                                              option should not be specified.

--lm_head_precision            &lt;VAL&gt;          Precision for lm_head (output.weight) tensor. &quot;FP_32&quot; supported.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--export_tokenizer_json</span></code> option supports tokenizers for QWen-1, BaiChuan-1, and Mistral models.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">--config_file</span></code> option is not provided, the composer will access the config.json file in the downloaded model path given in <code class="docutils literal notranslate"><span class="pre">--model</span></code> option and accordingly will fetch the generic configuration file from <code class="docutils literal notranslate"><span class="pre">QNN_SDK_ROOT</span></code> path.</p>
</div>
<div class="section" id="generic-configuration-file-explanation">
<span id="id2"></span><h2>Generic Configuration File Explanation<a class="headerlink" href="#generic-configuration-file-explanation" title="Permalink to this heading">¶</a></h2>
<p>Configuration.json will be a JSON file that will provide information about the model to the <code class="docutils literal notranslate"><span class="pre">qnn-genai-transformer-composer</span></code>, to prepare the model for inferencing via Genie.</p>
<div class="section" id="model-params-description">
<span id="generic-configuration-file-param-explanation"></span><h3>Model Params Description<a class="headerlink" href="#model-params-description" title="Permalink to this heading">¶</a></h3>
<p>There are 26 static params of the model under 5 categories of params:</p>
<ul class="simple">
<li><p>General parameters provide global information about the model.</p></li>
<li><p>Size parameters inform about the main model dimensions.</p></li>
<li><p>Architecture parameters provide information about the transformer control flow.</p></li>
<li><p>Operation parameters conveys operator details.</p></li>
<li><p>Tensor parameters provide details about the model tensors.</p></li>
</ul>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 35%" />
<col style="width: 15%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Param-Name</p></th>
<th class="head"><p>DataType</p></th>
<th class="head"><p>Param-Description</p></th>
<th class="head"><p>Optional /Mandatory</p></th>
<th class="head"><p>Possible values</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>general.name</p></td>
<td><p>String</p></td>
<td><p>Model name in a readable form</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>general.architecture</p></td>
<td><p>String</p></td>
<td><p>Model global architecture</p></td>
<td><p>Optional</p></td>
<td><p>generic , llama, qwen, gpt2. Default: generic</p></td>
</tr>
<tr class="row-even"><td><p>general.tokenizer</p></td>
<td><p>String</p></td>
<td><p>Tokenizer to use</p></td>
<td><p>Optional</p></td>
<td><p>none, gpt2, llama, tiktoken. Default: none</p></td>
</tr>
<tr class="row-odd"><td><p>general.alignment</p></td>
<td><p>Integer</p></td>
<td><p>Byte alignment for each tensor within the file</p></td>
<td><p>Optional</p></td>
<td><p>Default: 32</p></td>
</tr>
<tr class="row-even"><td><p>general.hf_hub_model_id</p></td>
<td><p>String</p></td>
<td><p>specifying model identifier in the HuggingFace repository</p></td>
<td><p>Optional</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>general.output</p></td>
<td><p>String</p></td>
<td><p>Model’s output</p></td>
<td><p>Optional</p></td>
<td><p>logits, embeddings. Default: logits</p></td>
</tr>
<tr class="row-even"><td><p>size.vocabulary</p></td>
<td><p>Integer</p></td>
<td><p>Vocabulary size.</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>size.context</p></td>
<td><p>Integer</p></td>
<td><p>Maximum number of tokens the transformer will consider to predict the next token</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>size.embedding</p></td>
<td><p>Integer</p></td>
<td><p>Length of the embedding vector representing a token</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>size.embedding_per_head</p></td>
<td><p>Integer</p></td>
<td><p>Length of the embedding vector per head</p></td>
<td><p>Optional</p></td>
<td><p>Default: size.embedding / architecture.num_head</p></td>
</tr>
<tr class="row-even"><td><p>size.feedforward</p></td>
<td><p>Integer</p></td>
<td><p>Size of the inner layer within the feed-forward network</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>architecture.num_decoders</p></td>
<td><p>Integer</p></td>
<td><p>Number of decoder layers</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>architecture.num_heads</p></td>
<td><p>Integer</p></td>
<td><p>Number of attention heads</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>architecture.num_kv_heads</p></td>
<td><p>Integer</p></td>
<td><p>Number of attention heads for keys-values incase of grouped query attention</p></td>
<td><p>Optional</p></td>
<td><p>Default: architecture.num_heads</p></td>
</tr>
<tr class="row-even"><td><p>architecture.connector</p></td>
<td><p>String</p></td>
<td><p>How the attention and feed-forward networks are connected to each other</p></td>
<td><p>Mandatory</p></td>
<td><p>sequential, parallel</p></td>
</tr>
<tr class="row-odd"><td><p>architecture.gating</p></td>
<td><p>String</p></td>
<td><p>Gating type of the transformer.</p></td>
<td><p>Mandatory</p></td>
<td><p>gated, fully-connected</p></td>
</tr>
<tr class="row-even"><td><p>operation.normalization</p></td>
<td><p>String</p></td>
<td><p>Normalization operator</p></td>
<td><p>Mandatory</p></td>
<td><p>layernorm, RMS-norm</p></td>
</tr>
<tr class="row-odd"><td><p>operation.activation</p></td>
<td><p>String</p></td>
<td><p>Non-linear activation operator for feed-forward</p></td>
<td><p>Mandatory</p></td>
<td><p>ReLU, GeLU, SiLU</p></td>
</tr>
<tr class="row-even"><td><p>operation.positional_embedding</p></td>
<td><p>String</p></td>
<td><p>How positional information is handled</p></td>
<td><p>Mandatory</p></td>
<td><p>WPE, RoPE</p></td>
</tr>
<tr class="row-odd"><td><p>operation.rope_num_rotations</p></td>
<td><p>Integer</p></td>
<td><p>Number of elements to be affected by the rope operation</p></td>
<td><p>Mandatory with “RoPE”</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>operation.rope_complex_organization</p></td>
<td><p>String</p></td>
<td><p>How RoPE real and imaginary parts are expected to be organized in memory</p></td>
<td><p>Mandatory with “RoPE” without “tensor.kq_complex_organization” specified</p></td>
<td><p>AoS, SoA</p></td>
</tr>
<tr class="row-odd"><td><p>operation.rope_scaling</p></td>
<td><p>Floating point</p></td>
<td><p>Scaling factor for the RoPE operator</p></td>
<td><p>Optional with “RoPE”</p></td>
<td><p>Default: 10000.0f</p></td>
</tr>
<tr class="row-even"><td><p>operation.rope.scaling.config</p></td>
<td><p>Dictionary</p></td>
<td><p>RoPE Scaling config</p></td>
<td><p>Optional with “RoPE”</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>operation.rope.scaling.factor</p></td>
<td><p>Array</p></td>
<td><p>An array of rope scaling factors</p></td>
<td><p>Optional</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>operation.normalization_epsilon</p></td>
<td><p>Floating point</p></td>
<td><p>Epsilon for the normalization operator</p></td>
<td><p>Optional</p></td>
<td><p>Default: 0.000001</p></td>
</tr>
<tr class="row-odd"><td><p>operation.attention_mode</p></td>
<td><p>String</p></td>
<td><p>How the model attends to previous and future tokens</p></td>
<td><p>Optional</p></td>
<td><p>causal, bidirectional. Default: causal</p></td>
</tr>
<tr class="row-even"><td><p>tensor.layer_name</p></td>
<td><p>String</p></td>
<td><p>The layer name prefix for layer tensors. “(d+)” regex for the layer number.</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>tensor.kq_complex_organization</p></td>
<td><p>String</p></td>
<td><p>If RoPE real and imaginary parts are expected to be SoA convert them to AoS</p></td>
<td><p>Mandatory with “RoPE” without “operation.rope_complex_organization” specified</p></td>
<td><p>SoA</p></td>
</tr>
<tr class="row-even"><td><p>name</p></td>
<td><p>String</p></td>
<td><p>Tensor name used in the model file</p></td>
<td><p>Mandatory</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>transposed</p></td>
<td><p>Boolean</p></td>
<td><p>Whether the tensor is transposed or not with respect to the standard matrix multiplication convention in linear algebra when the matrix is at the rigth hand side of the matrix multiplication</p></td>
<td><p>Optional</p></td>
<td><p>Default: False</p></td>
</tr>
<tr class="row-even"><td><p>scale</p></td>
<td><p>Floating point</p></td>
<td><p>Multiplicative coefficient to be applied to each tensor element before processing</p></td>
<td><p>Optional</p></td>
<td><p>Default: 1.0</p></td>
</tr>
<tr class="row-odd"><td><p>offset</p></td>
<td><p>Floating point</p></td>
<td><p>Additive coefficient to be applied to each tensor element before processing</p></td>
<td><p>Optional</p></td>
<td><p>Default: 0.0</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="model-tensor-description">
<span id="generic-configuration-file-tensor-explanation"></span><h3>Model Tensor Description<a class="headerlink" href="#model-tensor-description" title="Permalink to this heading">¶</a></h3>
<p>Tensor normalized names are of the form tensor.identifier_weight and tensor.identifier_bias where “identifier” is one of the following:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 20%" />
<col style="width: 65%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Tensor name</p></th>
<th class="head"><p>Tensor Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>embedding_token</p></td>
<td><p>Model Tensor</p></td>
<td><p>Word token embedding tensor</p></td>
</tr>
<tr class="row-odd"><td><p>embedding_position</p></td>
<td><p>Model Tensor</p></td>
<td><p>Word position embedding tensor (not expected with “RoPE”)</p></td>
</tr>
<tr class="row-even"><td><p>embedding_token_type</p></td>
<td><p>Model Tensor</p></td>
<td><p>Segment distinction within the input sequence</p></td>
</tr>
<tr class="row-odd"><td><p>embedding_normalization</p></td>
<td><p>Model Tensor</p></td>
<td><p>Embedding normalization tensor (expected with “sequential_post_layer_normalization”)</p></td>
</tr>
<tr class="row-even"><td><p>attention_normalization</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Attention input normalization tensor</p></td>
</tr>
<tr class="row-odd"><td><p>attention_q</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Attention query tensor</p></td>
</tr>
<tr class="row-even"><td><p>attention_k</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Attention key tensor</p></td>
</tr>
<tr class="row-odd"><td><p>attention_v</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Attention value tensor</p></td>
</tr>
<tr class="row-even"><td><p>attention_qkv</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Fused attention query-key-value tensor</p></td>
</tr>
<tr class="row-odd"><td><p>attention_output</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Attention output or projection tensor</p></td>
</tr>
<tr class="row-even"><td><p>feedforward_normalization</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feed-forward input or post-attention normalization tensor (not expected when connector is “parallel”)</p></td>
</tr>
<tr class="row-odd"><td><p>feedforward_gate</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feed-forward gate or fully-connected layer tensor</p></td>
</tr>
<tr class="row-even"><td><p>feedforward_up</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feed-forward up tensor (not expected with “fully-connected”)</p></td>
</tr>
<tr class="row-odd"><td><p>feedforward_up_gate</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feed-forward up and gate tensor (not expected with “fully-connected”)</p></td>
</tr>
<tr class="row-even"><td><p>feedforward_output</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feedforward output or projection tensor or down tensor</p></td>
</tr>
<tr class="row-odd"><td><p>feedforward_output_normalization</p></td>
<td><p>Layer Tensor</p></td>
<td><p>Feedforward output normalization tensor</p></td>
</tr>
<tr class="row-even"><td><p>output_normalization</p></td>
<td><p>Model Tensor</p></td>
<td><p>Output normalization tensor (not expected with “sequential_post_layer_normalization”)</p></td>
</tr>
<tr class="row-odd"><td><p>output</p></td>
<td><p>Model Tensor</p></td>
<td><p>Output tensor</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="rope-scaling-config-description">
<h3>RoPE Scaling Config Description<a class="headerlink" href="#rope-scaling-config-description" title="Permalink to this heading">¶</a></h3>
<p>The operation.rope.scaling.config key in the Model params table is a dictionary containing the keys depending on the RoPE type.
Following section describes the keys for different rope types.</p>
<div class="section" id="rope-type-llama3">
<h4>Rope Type <em>llama3</em><a class="headerlink" href="#rope-type-llama3" title="Permalink to this heading">¶</a></h4>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Mandatory</p></th>
<th class="head"><p>DefaultValue</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>factor</p></td>
<td><p>Floating point</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>high_freq_factor</p></td>
<td><p>Floating point</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>low_freq_factor</p></td>
<td><p>Floating point</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>original_max_position_embeddings</p></td>
<td><p>Integer</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>type</p></td>
<td><p>String</p></td>
<td><p>True</p></td>
<td><p>llama3</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="rope-type-yarn">
<h4>Rope Type <em>yarn</em><a class="headerlink" href="#rope-type-yarn" title="Permalink to this heading">¶</a></h4>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Mandatory</p></th>
<th class="head"><p>DefaultValue</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>factor</p></td>
<td><p>Floating point</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>original_max_position_embeddings</p></td>
<td><p>Integer</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>attention_factor</p></td>
<td><p>Floating point</p></td>
<td><p>False</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>beta_fast</p></td>
<td><p>Floating point</p></td>
<td><p>False</p></td>
<td><p>32.0</p></td>
</tr>
<tr class="row-even"><td><p>beta_slow</p></td>
<td><p>Floating point</p></td>
<td><p>False</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p>type</p></td>
<td><p>String</p></td>
<td><p>True</p></td>
<td><p>yarn</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="rope-type-longrope">
<h4>Rope Type <em>longrope</em><a class="headerlink" href="#rope-type-longrope" title="Permalink to this heading">¶</a></h4>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Mandatory</p></th>
<th class="head"><p>DefaultValue</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>long_factor</p></td>
<td><p>Array</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>short_factor</p></td>
<td><p>Array</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>original_max_position_embeddings</p></td>
<td><p>Integer</p></td>
<td><p>True</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>attention_factor</p></td>
<td><p>Floating point</p></td>
<td><p>False</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>type</p></td>
<td><p>String</p></td>
<td><p>True</p></td>
<td><p>longrope</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="genie-app.html" class="btn btn-neutral float-left" title="genie-app" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2024, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>