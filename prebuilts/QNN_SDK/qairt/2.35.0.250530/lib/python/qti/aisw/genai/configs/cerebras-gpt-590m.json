{
    "general.name": "cerebras-gpt-590m",
    "general.architecture": "gpt2",
    "general.tokenizer": "gpt2",
    "general.hf_hub_model_id": "cerebras/Cerebras-GPT-590M",
    "general.alignment" : 128,
    "size.vocabulary": 50257,
    "size.context": 2048,
    "size.embedding": 1536,
    "size.feedforward": 6144,
    "architecture.num_decoders": 18,
    "architecture.num_heads": 12,
    "architecture.connector": "sequential_pre_layer_normalization",
    "architecture.gating": "fully-connected",
    "operation.normalization": "layernorm",
    "operation.normalization_epsilon": 1e-05,
    "operation.activation": "GeLU",
    "operation.positional_embedding": "WPE",
    "tensor.layer_name": "transformer.h.(\\d+).",
    "tensor.embedding_token_weight": {
        "name": "transformer.wte.weight"
    },
    "tensor.embedding_position_weight": {
        "name": "transformer.wpe.weight"
    },
    "tensor.attention_normalization_weight": {
        "name": "ln_1.weight"
    },
    "tensor.attention_normalization_bias": {
        "name": "ln_1.bias"
    },
    "tensor.attention_qkv_weight": {
        "name": "attn.c_attn.weight"
    },
    "tensor.attention_qkv_bias": {
        "name": "attn.c_attn.bias"
    },
    "tensor.attention_output_weight": {
        "name": "attn.c_proj.weight"
    },
    "tensor.attention_output_bias": {
        "name": "attn.c_proj.bias"
    },
    "tensor.feedforward_normalization_weight": {
        "name": "ln_2.weight"
    },
    "tensor.feedforward_normalization_bias": {
        "name": "ln_2.bias"
    },
    "tensor.feedforward_gate_weight": {
        "name": "mlp.c_fc.weight"
    },
    "tensor.feedforward_gate_bias": {
        "name": "mlp.c_fc.bias"
    },
    "tensor.feedforward_output_weight": {
        "name": "mlp.c_proj.weight"
    },
    "tensor.feedforward_output_bias": {
        "name": "mlp.c_proj.bias"
    },
    "tensor.output_normalization_weight": {
        "name": "transformer.ln_f.weight"
    },
    "tensor.output_normalization_bias": {
        "name": "transformer.ln_f.bias"
    }
}