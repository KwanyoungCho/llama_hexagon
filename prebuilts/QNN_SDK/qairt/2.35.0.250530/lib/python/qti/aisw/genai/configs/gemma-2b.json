{
    "general.name": "gemma-2b",
    "general.architecture": "generic",
    "general.tokenizer": "tiktoken",
    "general.hf_hub_model_id": "google/gemma-2b",
    "general.alignment" : 32,
    "size.vocabulary" : 256000,
    "size.context" : 8192,
    "size.embedding" : 2048,
    "size.feedforward" : 16384,
    "architecture.num_decoders" : 18,
    "architecture.num_heads" : 8,
    "architecture.num_kv_heads" : 1,
    "architecture.connector": "sequential_pre_layer_normalization",
    "architecture.gating" : "gated",
    "operation.normalization" : "RMS-norm",
    "operation.normalization_epsilon" : 1e-6,
    "operation.activation" : "GeLU",
    "operation.positional_embedding" : "RoPE",
    "tensor.kq_complex_organization" : "SoA",
    "operation.rope_num_rotations" : 256,
    "operation.rope_scaling" : 10000.0,
    "tensor.layer_name": "model.layers.(\\d+).",
    "tensor.embedding_token_weight": {
        "name": "model.embed_tokens.weight",
        "scale": 45.254834 
    },
    "tensor.attention_normalization_weight": {
        "name": "input_layernorm.weight",
        "offset": 1
    },
    "tensor.attention_q_weight": {
        "name": "self_attn.q_proj.weight",
        "transposed": true
    },
    "tensor.attention_k_weight": {
        "name": "self_attn.k_proj.weight",
        "transposed": true
    },
    "tensor.attention_v_weight": {
        "name": "self_attn.v_proj.weight",
        "transposed": true
    },
    "tensor.attention_output_weight": {
        "name": "self_attn.o_proj.weight",
        "transposed": true
    },
    "tensor.feedforward_normalization_weight": {
        "name": "post_attention_layernorm.weight",
        "offset": 1
    },
    "tensor.feedforward_gate_weight": {
        "name": "mlp.gate_proj.weight",
        "transposed": true
    },
    "tensor.feedforward_up_weight": {
        "name": "mlp.up_proj.weight",
        "transposed": true
    },
    "tensor.feedforward_output_weight": {
        "name": "mlp.down_proj.weight",
        "transposed": true
    },
    "tensor.output_normalization_weight": {
        "name": "model.norm.weight",
        "offset": 1
    }
}