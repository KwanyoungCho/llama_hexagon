# ==============================================================================
#
#  Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.
#  All rights reserved.
#  Confidential and Proprietary - Qualcomm Technologies, Inc.
#
# ==============================================================================

import json
import os

import numpy as np
import onnx
import yaml
from safetensors.numpy import save_file

from qairt.api.configs.common import AISWBaseModel


class LoraAdapter(AISWBaseModel):
    """
    LoRA adapter definition.
    """

    name: str
    """
    Adapter use case
    """

    weights: dict[str, np.ndarray]
    """
    A dictionary mapping lora tensor name to numpy.ndarray for the adapter
    """

    encodings: dict = {}
    """
    Tensor quantization encodings for the lora adapter
    """

    class Config:
        arbitrary_types_allowed = True


class Mha2ShaTransformedModel(AISWBaseModel):
    """
    A dataclass that holds references to all the artifacts generated by running MHA2SHA transformations
    """

    model: onnx.ModelProto
    """
    Transformed SHA model
    """

    lora_weights: dict[str, np.ndarray] = {}
    """
    A dictionary mapping lora tensors names in the base model to weights (numpy.ndarray)
    """

    encodings: dict = {}
    """
    Tensor quantization encodings for the transformed SHA model
    """

    encodings_map: dict = {}
    """
    A dictionary mapping the tensor names from the MHA model to a list of tensors in the
    transformed SHA model
    """

    lora_adapters: list[LoraAdapter] = []
    """
    A list of LoRA adapters for the transformed SHA model
    There LoRA adapters are derived from the original LoRA adapters of the MHA model
    """

    lora_tensor_names: set[str] = set()
    """
    A set of tensor names assocciated with LoRA adapters in the transformed model
    """

    class Config:
        arbitrary_types_allowed = True

    def export(self, path: str, model_name: str, save_model_file: bool = True):
        f"""
        Export MHA2SHA generated model artifacts
        Args:
            path(str): Directory where the artifacts are to be saved.
            model_name(str): Prefix to model and artifact file names.
            save_model_file(bool): Whether the SHA model should be exported.
                                   Useful when calling from a function that further modifies the model,
                                   but when the other artifacts need to be saved
        """

        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)

        elif not os.path.isdir(path):
            raise ValueError(f"{path} is not a directory")

        if save_model_file:
            model_path = os.path.join(path, f"{model_name}.onnx")
            onnx.save(self.model, model_path, save_as_external_data=True, all_tensors_to_one_file=True)

        if self.encodings:
            encodings_path = os.path.join(path, f"{model_name}.encodings")
            with open(encodings_path, "w") as f:
                f.write(json.dumps(self.encodings, indent=4))

            encodings_mapping_file_path = os.path.join(path, "mha_to_sha_encodings_mapping.json")
            with open(encodings_mapping_file_path, "w") as f:
                f.write(json.dumps(self.encodings_map, indent=4))

        if self.lora_adapters:
            use_cases = []

            for adapter in self.lora_adapters:
                safetensors_path = os.path.join(path, f"{adapter.name}.safetensors")
                save_file(adapter.weights, safetensors_path)

                encodings_path = os.path.join(path, f"{adapter.name}.encodings")
                with open(encodings_path, "w") as f:
                    f.write(json.dumps(adapter.encodings))

                use_cases.append(
                    {
                        "name": adapter.name,
                        "model_name": os.path.abspath(model_path),
                        "lora_weights": os.path.abspath(safetensors_path),
                        "quant_overrides": os.path.abspath(encodings_path),
                        "output_path": "../lora_output",
                    }
                )

            lora_config = {"use_case": use_cases}

            lora_config_path = os.path.join(path, "lora_importer_config.yaml")
            with open(lora_config_path, "w") as f:
                f.write(yaml.dump(lora_config))

        if self.lora_tensor_names:
            lora_tensor_names_path = os.path.join(path, "lora_tensor_names.txt")
            with open(lora_tensor_names_path, "w") as f:
                for tensor in self.lora_tensor_names:
                    f.write(f"{tensor}\n")

