# Q4_0 Per-Group Int8 MatMul with NPU-CPU Hybrid Architecture

## ğŸ¯ ëª©í‘œ
- Q4_0 quantized weight Ã— FP32 activationì˜ row ë‹¨ìœ„ per-group quantization ì§€ì›
- NPUì˜ int8 í–‰ë ¬ê³±ê³¼ CPUì˜ scale ì²˜ë¦¬ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìµœì  ì„±ëŠ¥ ë‹¬ì„±
- Element-wise ì—°ì‚°ì„ í†µí•œ ì •í™•í•œ dequantizationê³¼ groupë³„ accumulation

## ğŸ“Š í•µì‹¬ ì•„í‚¤í…ì²˜

### ê¸°ë³¸ ì ‘ê·¼ë²•
```
Weight (Q4_0 per-group) Ã— Activation (FP32) = Output (FP32)
```

### Row ë‹¨ìœ„ Group ë¶„í• 
- **Weight**: Row ë‹¨ìœ„ë¡œ group ë¶„í•  (ê° group = 32 elements)
- **Activation**: Column ìœ ì§€, Weight groupì— ë§ì¶° row ë¶„í• 
- **Scale Matrix**: Weightì˜ ê° groupì— ëŒ€ì‘í•˜ëŠ” scale ê°’

## ğŸ”„ ì—°ì‚° Flow

### 1. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„
```
Input:
- Weight: Q4_0 format (quantized values + scales per group)
- Activation: FP32 format (transposeëœ ìƒíƒœë¡œ ì €ì¥)

Preparation:
- Weightë¥¼ row ë‹¨ìœ„ groupìœ¼ë¡œ ë¶„í• 
- ê° groupì˜ quantized values ì¶”ì¶œ
- ê° groupì˜ scale ê°’ìœ¼ë¡œ scale matrix ìƒì„±
```

### 2. NPU Int8 í–‰ë ¬ê³± ìˆ˜í–‰
```
For each group:
  NPU: int8_weight_group Ã— int8_activation_group = int32_result_group
```

### 3. CPU Scale Matrix ìƒì„± ë° ì ìš©
```
CPU: weight_scales Ã— activation_scales = combined_scales_matrix
NPU: int32_result âŠ— combined_scales_matrix = fp32_dequantized_result
```

### 4. Groupë³„ Accumulation
```
NPU: group_results[0] âŠ• group_results[1] âŠ• ... âŠ• group_results[n] = final_output
```

## ğŸ“‹ êµ¬í˜„ ê³„íš

### **Phase 1: ë°ì´í„° êµ¬ì¡° ë° Group ë¶„í• **
- [ ] Q4_0 group tiling êµ¬ì¡°ì²´ ì„¤ê³„
- [ ] Row ë‹¨ìœ„ group ë¶„í•  ì•Œê³ ë¦¬ì¦˜
- [ ] Scale matrix ìƒì„± ë¡œì§

### **Phase 2: NPU Int8 í–‰ë ¬ê³± ì—”ì§„**
- [ ] Groupë³„ int8 matmul QNN graph ìƒì„±
- [ ] Batch processing ìµœì í™”
- [ ] Memory layout ìµœì í™”

### **Phase 3: CPU Scale Matrix ìƒì„±**
- [ ] Per-group scale ì¶”ì¶œ
- [ ] Broadcasting ìµœì í™”
- [ ] CPU-NPU ë™ê¸°í™”

### **Phase 4: NPU Element-wise ì—°ì‚°**
- [ ] Dequantization (int32 â†’ fp32 conversion)
- [ ] Element-wise multiplication
- [ ] Element-wise accumulation

### **Phase 5: ë©”ì¸ í†µí•© í•¨ìˆ˜**
- [ ] End-to-end pipeline êµ¬í˜„
- [ ] Error handling ë° validation
- [ ] Performance profiling

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ (8ì£¼)

| ì£¼ì°¨ | Phase | ì£¼ìš” ì‘ì—… | ì‚°ì¶œë¬¼ |
|------|-------|-----------|---------|
| **Week 1-2** | Phase 1 | ê¸°ë³¸ ì¸í”„ë¼ êµ¬ì¶• | Group ë¶„í•  ì•Œê³ ë¦¬ì¦˜, ë°ì´í„° êµ¬ì¡° |
| **Week 3-4** | Phase 2 | NPU ì—°ì‚° ì—”ì§„ | Int8 matmul QNN graph |
| **Week 5-6** | Phase 3-4 | CPU scale + NPU element-wise | Scale matrix ìƒì„±, Element-wise ì—°ì‚° |
| **Week 7-8** | Phase 5 | í†µí•© ë° ìµœì í™” | ì™„ì „í•œ Q4_0 int8 matmul í•¨ìˆ˜ |

## ğŸ” ê¸°ì¡´ Hexagon Backend FP32 MatMul ë¶„ì„

### **QNN SDK API ì‚¬ìš© íŒ¨í„´ ë¶„ì„**

#### **1. QNN Instance ê´€ë¦¬ êµ¬ì¡°**
```cpp
class qnn_instance {
    // í•µì‹¬ QNN í•¸ë“¤ë“¤
    Qnn_GraphHandle_t _qnn_graph_handle;
    Qnn_ContextHandle_t _qnn_context_handle;
    Qnn_BackendHandle_t _qnn_backend_handle;
    Qnn_DeviceHandle_t _qnn_device_handle;
    
    // ë™ì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì¸í„°í˜ì´ìŠ¤
    QNN_INTERFACE_VER_TYPE _qnn_raw_interface;
    qnn_interface _qnn_interface;  // Function pointer wrapper
};
```

#### **2. QNN Graph ìƒì„± íŒ¨í„´**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:3706-3800
int qnn_instance::init_qnn_graph(const std::string & graph_name, 
                                  HEXAGONBackend device, 
                                  size_t vtcm_size_in_mb, 
                                  size_t hvx_threads) {
    // 1. Graph ìƒì„±
    error = _qnn_raw_interface.graphCreate(_qnn_context_handle, 
                                          graph_name.c_str(), 
                                          NULL, &_qnn_graph_handle);
    
    // 2. HTP ì„±ëŠ¥ ì„¤ì •
    htp_set_memory_grow_size(size);
    htp_set_n_hvx_threads(n_threads);
    
    return QNN_SUCCESS;
}
```

#### **3. QNN Tensor ìƒì„± í•µì‹¬ í•¨ìˆ˜**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:4089-4170
static Qnn_Tensor_t * ggmlqnn_create_general_tensor(
    qnn_instance * instance, 
    Qnn_GraphHandle_t graph_handle,
    const ggml_tensor * tensor, 
    const char * name,
    Qnn_TensorType_t qnn_tensor_type,    // APP_WRITE/APP_READ/NATIVE
    Qnn_DataType_t qnn_data_type,        // FLOAT_32/INT8_t/etc
    uint32_t rank, 
    uint32_t * dims,
    void * data, 
    uint32_t data_size) {
    
    Qnn_Tensor_t qnn_tensor = {
        .version = QNN_TENSOR_VERSION_1,
        .v1 = {
            .id = 0,
            .name = tensor_name,
            .type = qnn_tensor_type,
            .dataFormat = QNN_TENSOR_DATA_FORMAT_FLAT_BUFFER,
            .dataType = qnn_data_type,
            .rank = rank,
            .dimensions = tensor_dims,
            .memType = QNN_TENSORMEMTYPE_RAW,  // ë˜ëŠ” MEMHANDLE
            .clientBuf = {.data = data, .dataSize = data_size}
        }
    };
    
    // QNN Graphì— ë“±ë¡
    _qnn_interface.tensorCreateGraphTensor(graph_handle, p_qnn_tensor);
    return p_qnn_tensor;
}
```

### **ION ë©”ëª¨ë¦¬ ê´€ë¦¬ ìƒì„¸ ë¶„ì„**

#### **1. ION ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œ ì´ìœ  í™•ì¸ë¨**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:3078-3130
// QNN SDKëŠ” NPU ì‚¬ìš© ì‹œ ION ë©”ëª¨ë¦¬ í•„ìˆ˜ ì‚¬ìš©
Qnn_MemDescriptor_t descriptor = {
    {rank, dimensions, nullptr},
    data_type, 
    QNN_MEM_TYPE_ION,      // â† ION ë©”ëª¨ë¦¬ íƒ€ì… í•„ìˆ˜!
    {{mem_fd}}             // File descriptor í•„ìš”
};
```

#### **2. RPC ë©”ëª¨ë¦¬ í• ë‹¹ íŒ¨í„´**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:2960-2995
void * qnn_instance::alloc_rpcmem(size_t bytes, size_t alignment) {
    // 1. RPC ë©”ëª¨ë¦¬ í• ë‹¹ (ION ê¸°ë°˜)
    void * buf = _pfn_rpc_mem_alloc(RPCMEM_HEAP_ID_SYSTEM, 
                                   RPCMEM_DEFAULT_FLAGS, 
                                   allocate_bytes);
    
    // 2. ì •ë ¬ëœ í¬ì¸í„° ê³„ì‚°
    auto aligned_buf = reinterpret_cast<void *>(
        ggmlqnn_align_to(alignment, reinterpret_cast<intptr_t>(buf)));
    
    // 3. ë§¤í•‘ í…Œì´ë¸”ì— ì €ì¥
    _rpcmem_store_map.insert({aligned_buf, buf});
    _rpcmem_usage_map.insert({aligned_buf, bytes});
    
    return aligned_buf;
}
```

#### **3. ION ë©”ëª¨ë¦¬ ë“±ë¡ ê³¼ì •**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:3045-3090
Qnn_MemHandle_t qnn_instance::register_rpcmem(void * p_data, 
                                              const uint32_t rank, 
                                              uint32_t * dimensions, 
                                              Qnn_DataType_t data_type) {
    // 1. RPC ë©”ëª¨ë¦¬ë¥¼ File Descriptorë¡œ ë³€í™˜
    int32_t mem_fd = rpcmem_to_fd(p_data);
    
    // 2. QNN Memory Descriptor ìƒì„±
    Qnn_MemDescriptor_t descriptor = {
        {rank, dimensions, nullptr},
        data_type, 
        QNN_MEM_TYPE_ION,
        {{mem_fd}}
    };
    
    // 3. QNN Contextì— ë©”ëª¨ë¦¬ ë“±ë¡
    error = _qnn_interface.qnn_mem_register(_qnn_context_handle, 
                                           &descriptor, 1, &handle);
    
    return handle;
}
```

### **QNN Graph ì‹¤í–‰ íë¦„ ë¶„ì„**

#### **1. Element-wise ì—°ì‚° ì˜ˆì œ (Add/Mul/etc)**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:4206-4358
static void ggmlqnn_compute_elementwise(ggml_backend_hexagon_context * ctx, 
                                        ggml_tensor * op) {
    // Graph ìºì‹± í™•ì¸
    if (ctx->qnn_singlenode_graph_map.find(graph_name) != 
        ctx->qnn_singlenode_graph_map.end()) {
        // ê¸°ì¡´ Graph ì¬ì‚¬ìš©
        graph_handle = std::get<0>(graph_item);
        // Tensorë“¤ ì¬ì‚¬ìš©
    } else {
        // ìƒˆ Graph ìƒì„±
        // 1. Tensor ìƒì„±
        p_tensor0 = ggmlqnn_create_compute_tensor(instance, graph_handle, src0, 
                                                 QNN_TENSOR_TYPE_APP_WRITE);
        p_tensor1 = ggmlqnn_create_compute_tensor(instance, graph_handle, src1, 
                                                 QNN_TENSOR_TYPE_APP_WRITE);
        p_tensor2 = ggmlqnn_create_compute_tensor(instance, graph_handle, dst, 
                                                 QNN_TENSOR_TYPE_APP_READ);
        
        // 2. Op Config ìƒì„±
        Qnn_OpConfig_t op_config = ggmlqnn_create_op_config(
            ggml_op_name, QNN_OP_PACKAGE_NAME_QTI_AISW, qnn_op_name, 
            nullptr, 0, input_tensors.data(), input_param_count, 
            output_tensors, 1);
        
        // 3. Graphì— Node ì¶”ê°€
        _qnn_interface.graphAddNode(graph_handle, op_config);
        
        // 4. Graph Finalize
        _qnn_interface.graphFinalize(graph_handle, nullptr, nullptr);
        
        // 5. Graph ìºì‹±
        ctx->qnn_singlenode_graph_map[graph_name] = 
            std::make_tuple(graph_handle, qnn_elementwise_tensors);
    }
    
    // ë°ì´í„° ì„¤ì •
    if (enable_npu_rpc) {
        // ION ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ
        uint8_t * qnn_buffer = instance->get_rpcmem_from_memhandle(
            QNN_VER_PTR(*p_tensor0)->memHandle);
        memcpy(qnn_buffer, src0->data, ggml_nbytes(src0));
    } else {
        // ì¼ë°˜ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ
        QNN_VER_PTR(*p_tensor0)->clientBuf = {src0->data, data_size};
    }
    
    // Graph ì‹¤í–‰
    _qnn_interface.graphExecute(graph_handle, input_tensors.data(), 
                               input_param_count, output_tensors, 1, 
                               nullptr, nullptr);
}
```

#### **2. Matrix Multiplication ë³µì¡í•œ ì˜ˆì œ**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:4359-4590 (4D MatMul)
// ë³µì¡í•œ transpose, reshape, tile ì—°ì‚°ë“¤ì„ ì¡°í•©
// 1. Reshape: [B0, H0, M, K] â†’ [B0, M, K]
// 2. Tile: [B0, M, K] â†’ [B1, M, K] (broadcasting)
// 3. Permute: [B1, H1, N, K] â†’ [B1, H1, K, N]
// 4. MatMul: [B1, M, K] Ã— [B1, K, N] â†’ [B1, M, N]
// 5. Reshape: [B1, M, N] â†’ [B1, H1, M, N]
```

### **ë©”ëª¨ë¦¬ ê´€ë¦¬ Best Practices**

#### **1. Memory Alignment ì²˜ë¦¬**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:2234-2240
static intptr_t ggmlqnn_align_to(size_t alignment, intptr_t offset) {
    return ((offset + alignment - 1) / alignment) * alignment;
}
```

#### **2. Memory Pool ê´€ë¦¬**
```cpp
// RPC ë©”ëª¨ë¦¬ í’€ ìš©ëŸ‰ ì œí•œ ë° ì‚¬ìš©ëŸ‰ ì¶”ì 
if (_rpcmem_usage > (_rpcmem_capacity - (8 * SIZE_IN_MB))) {
    // 8MB ì—¬ìœ  ê³µê°„ í™•ë³´
    return nullptr;
}
```

#### **3. Resource ì •ë¦¬**
```cpp
// Memory handle í•´ì œ
void qnn_instance::unregister_rpcmem(Qnn_MemHandle_t mem_handle) {
    _qnn_interface.qnn_mem_de_register(&mem_handle, 1);
    _qnn_mem_set.erase(mem_handle);
}
```

## ğŸ’¡ NPU Int8 MatMul êµ¬í˜„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### **1. QNN Tensor Type ì „ëµ**
- **Input Weight/Activation**: `QNN_TENSOR_TYPE_APP_WRITE`
- **Intermediate Results**: `QNN_TENSOR_TYPE_NATIVE` 
- **Final Output**: `QNN_TENSOR_TYPE_APP_READ`

### **2. Memory Type ì„ íƒ**
- **ION ë©”ëª¨ë¦¬ í•„ìˆ˜**: NPU ì‚¬ìš© ì‹œ `QNN_TENSORMEMTYPE_MEMHANDLE`
- **ì¼ë°˜ ë©”ëª¨ë¦¬**: CPU ì‚¬ìš© ì‹œ `QNN_TENSORMEMTYPE_RAW`

### **3. Graph Caching ìµœì í™”**
```cpp
// Graph ì¬ì‚¬ìš©ìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
std::map<std::string, qnn_singlenode_res_t> qnn_singlenode_graph_map;
```

### **4. Error Handling íŒ¨í„´**
```cpp
#define CHECK_QNN_API(error, result) \
    do { \
        error = (result); \
        if (QNN_SUCCESS != error) { \
            GGMLHEXAGON_LOG_WARN("QNN API error = %d(%s)\n", \
                error, ggmlqnn_get_qnnerror_string(error)); \
        } \
    } while (0)
```

### **5. Performance Profiling êµ¬ì¡°**
```cpp
hexagon_perf op_perf(graph_name, ggml_original_opname, input_size, output_size);
op_perf.start();
// ... QNN operations ...
op_perf.info(); // ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ì¶œë ¥
```

## ğŸ” ê¸°ì¡´ Hexagon Backend FP32 MatMul ë¶„ì„

### **QNN SDK API ì‚¬ìš© íŒ¨í„´ ë¶„ì„**

#### **1. QNN Instance ê´€ë¦¬ êµ¬ì¡°**
```cpp
class qnn_instance {
    // í•µì‹¬ QNN í•¸ë“¤ë“¤
    Qnn_GraphHandle_t _qnn_graph_handle;
    Qnn_ContextHandle_t _qnn_context_handle;
    Qnn_BackendHandle_t _qnn_backend_handle;
    Qnn_DeviceHandle_t _qnn_device_handle;
    
    // ë™ì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì¸í„°í˜ì´ìŠ¤
    QNN_INTERFACE_VER_TYPE _qnn_raw_interface;
    qnn_interface _qnn_interface;  // Function pointer wrapper
};
```

#### **2. QNN Graph ìƒì„± íŒ¨í„´**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:3706-3800
int qnn_instance::init_qnn_graph(const std::string & graph_name, 
                                  HEXAGONBackend device, 
                                  size_t vtcm_size_in_mb, 
                                  size_t hvx_threads) {
    // 1. Graph ìƒì„±
    error = _qnn_raw_interface.graphCreate(_qnn_context_handle, 
                                          graph_name.c_str(), 
                                          NULL, &_qnn_graph_handle);
    
    // 2. HTP ì„±ëŠ¥ ì„¤ì •
    htp_set_memory_grow_size(size);
    htp_set_n_hvx_threads(n_threads);
    
    return QNN_SUCCESS;
}
```

#### **3. QNN Tensor ìƒì„± í•µì‹¬ í•¨ìˆ˜**
```cpp
// ìœ„ì¹˜: ggml-hexagon.cpp:4089-4170
static Qnn_Tensor_t * ggmlqnn_create_general_tensor(
    qnn_instance * instance, 
    Qnn_GraphHandle_t graph_handle,
    const ggml_tensor * tensor, 
    const char * name,
    Qnn_TensorType_t qnn_tensor_type,    // APP_WRITE/APP_READ/NATIVE
    Qnn_DataType_t qnn_data_type,        // FLOAT_32/INT8_t/etc
    uint32_t rank, 
    uint32_t * dims,
    void * data, 
    uint32_t data_size) {
    
    Qnn_Tensor_t qnn_tensor = {
        .version = QNN_TENSOR_VERSION_1,
        .v1 = {
            .name = tensor_name,
            .type = qnn_tensor_type,
            .dataFormat = QNN_TENSOR_DATA_FORMAT_FLAT_BUFFER,
            .dataType = qnn_data_type,
            .rank = rank,
            .dimensions = tensor_dims,
            .memType = QNN_TENSORMEMTYPE_RAW,  // ë˜ëŠ” MEMHANDLE
            .clientBuf = {.data = data, .dataSize = data_size}
        }
    };
    
    // QNN Graphì— ë“±ë¡
    _qnn_interface.tensorCreateGraphTensor(graph_handle, p_qnn_tensor);
    return p_qnn_tensor;
}
```

### **ION ë©”ëª¨ë¦¬ ê´€ë¦¬ ìƒì„¸ ë¶„ì„**

#### **1. ION ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œ ì´ìœ  í™•ì¸ë¨**
```cpp
// QNN SDKëŠ” NPU ì‚¬ìš© ì‹œ ION ë©”ëª¨ë¦¬ í•„ìˆ˜ ì‚¬ìš©
Qnn_MemDescriptor_t descriptor = {
    {rank, dimensions, nullptr},
    data_type, 
    QNN_MEM_TYPE_ION,      // â† ION ë©”ëª¨ë¦¬ íƒ€ì… í•„ìˆ˜!
    {{mem_fd}}             // File descriptor í•„ìš”
};
```

#### **2. RPC ë©”ëª¨ë¦¬ í• ë‹¹ íŒ¨í„´**
```cpp
// RPC ë©”ëª¨ë¦¬ = ION ë©”ëª¨ë¦¬
void * qnn_instance::alloc_rpcmem(size_t bytes, size_t alignment) {
    // 1. RPC ë©”ëª¨ë¦¬ í• ë‹¹ (ION ê¸°ë°˜)
    void * buf = _pfn_rpc_mem_alloc(RPCMEM_HEAP_ID_SYSTEM, 
                                   RPCMEM_DEFAULT_FLAGS, 
                                   allocate_bytes);
    
    // 2. ì •ë ¬ëœ í¬ì¸í„° ê³„ì‚°
    auto aligned_buf = reinterpret_cast<void *>(
        ggmlqnn_align_to(alignment, reinterpret_cast<intptr_t>(buf)));
    
    // 3. ë§¤í•‘ í…Œì´ë¸”ì— ì €ì¥
    _rpcmem_store_map.insert({aligned_buf, buf});
    
    return aligned_buf;
}
```

#### **3. ION ë©”ëª¨ë¦¬ ë“±ë¡ ê³¼ì •**
```cpp
Qnn_MemHandle_t qnn_instance::register_rpcmem(void * p_data, 
                                              const uint32_t rank, 
                                              uint32_t * dimensions, 
                                              Qnn_DataType_t data_type) {
    // 1. RPC ë©”ëª¨ë¦¬ë¥¼ File Descriptorë¡œ ë³€í™˜
    int32_t mem_fd = rpcmem_to_fd(p_data);
    
    // 2. QNN Memory Descriptor ìƒì„±
    Qnn_MemDescriptor_t descriptor = {
        {rank, dimensions, nullptr},
        data_type, QNN_MEM_TYPE_ION, {{mem_fd}}
    };
    
    // 3. QNN Contextì— ë©”ëª¨ë¦¬ ë“±ë¡
    error = _qnn_interface.qnn_mem_register(_qnn_context_handle, 
                                           &descriptor, 1, &handle);
    
    return handle;
}
```

### **QNN Graph ì‹¤í–‰ íë¦„ ë¶„ì„**

#### **1. Graph ìƒì„± â†’ ì‹¤í–‰ íŒ¨í„´**
```cpp
// 1. Graph ìºì‹± í™•ì¸
if (ctx->qnn_singlenode_graph_map.find(graph_name) != 
    ctx->qnn_singlenode_graph_map.end()) {
    // ê¸°ì¡´ Graph ì¬ì‚¬ìš©
} else {
    // 2. ìƒˆ Graph ìƒì„±
    // - Tensor ìƒì„± (ggmlqnn_create_compute_tensor)
    // - Op Config ìƒì„± (ggmlqnn_create_op_config)
    // - Graphì— Node ì¶”ê°€ (graphAddNode)
    // - Graph Finalize (graphFinalize)
    // - Graph ìºì‹±
}

// 3. ë°ì´í„° ì„¤ì •
if (enable_npu_rpc) {
    // ION ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ - memcpy í•„ìš”
    uint8_t * qnn_buffer = instance->get_rpcmem_from_memhandle(handle);
    memcpy(qnn_buffer, src_data, data_size);
} else {
    // ì¼ë°˜ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ - pointer ì„¤ì •
    QNN_VER_PTR(*tensor)->clientBuf = {src_data, data_size};
}

// 4. Graph ì‹¤í–‰
_qnn_interface.graphExecute(graph_handle, inputs, input_count, 
                           outputs, output_count, nullptr, nullptr);
```

### **ë©”ëª¨ë¦¬ ê´€ë¦¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸**

#### **1. ION ë©”ëª¨ë¦¬ í•„ìˆ˜ ì‚¬ìš© í™•ì¸**
- NPU backend ì‚¬ìš© ì‹œ `QNN_MEM_TYPE_ION` í•„ìˆ˜
- RPC ë©”ëª¨ë¦¬ = ION ë©”ëª¨ë¦¬ (Android/Linux í™˜ê²½)
- File descriptor ê¸°ë°˜ ë©”ëª¨ë¦¬ ê³µìœ 

#### **2. Memory Handle ê´€ë¦¬**
```cpp
// Memory handleê³¼ pointer ì–‘ë°©í–¥ ë§¤í•‘
std::unordered_map<void *, Qnn_MemHandle_t> _qnn_mem_set;
std::unordered_map<void *, Qnn_MemHandle_t> _qnn_rpc_buffer_to_handles;
```

#### **3. Memory Alignment ë° Pool ê´€ë¦¬**
```cpp
// ì •ë ¬ ì²˜ë¦¬
static intptr_t ggmlqnn_align_to(size_t alignment, intptr_t offset);

// Pool ì‚¬ìš©ëŸ‰ ì œí•œ (8MB ì—¬ìœ  ê³µê°„ í™•ë³´)
if (_rpcmem_usage > (_rpcmem_capacity - (8 * SIZE_IN_MB))) {
    return nullptr;
}
```

---

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„: Q4_0 Int8 MatMul êµ¬í˜„ ì‹œì‘**

ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬í˜„ì„ ì§„í–‰í•©ë‹ˆë‹¤:

1. **ê¸°ì¡´ QNN infrastructure í™œìš©**
2. **Group ë‹¨ìœ„ tiling ë¡œì§ êµ¬í˜„**  
3. **NPU int8 matmul Graph ìƒì„±**
4. **CPU scale matrix ì—°ì‚°ê³¼ NPU element-wise ì—°ì‚° í†µí•©**

ëª¨ë“  êµ¬í˜„ì€ ê¸°ì¡´ `ggmlqnn_compute_elementwise` ë° `ggmlqnn_compute_mul_mat` íŒ¨í„´ì„ ë”°ë¼ ì¼ê´€ì„± ìˆëŠ” ì½”ë“œ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•  ì˜ˆì •ì…ë‹ˆë‹¤.
